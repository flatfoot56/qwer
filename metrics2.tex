\documentclass[a4paper]{article}
\usepackage[14pt]{extsizes} % 
\usepackage[utf8]{inputenc}
\usepackage{setspace,amsmath}
\usepackage{mathtools}
\usepackage{pgfplots}
\usepackage{titlesec}
\usepackage{pdfpages}
\usepackage[shortlabels]{enumitem}
\usepackage{tikz}
\usetikzlibrary{angles,quotes}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{float}
\usepackage[section]{placeins}
\usepackage[makeroom]{cancel}
\usepackage{mathrsfs} % 
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
%\addto\captionsrussian{\renewcommand{\figurename}{Fig.}}
\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools} 
\newcommand*{\hm}[1]{#1\nobreak\discretionary{}
{\hbox{$\mathsurround=0pt #1$}}{}}
\usepackage{graphicx}  % 
\graphicspath{{images/}{images2/}}  % 
\setlength\fboxsep{3pt} %  \fbox{} 
\setlength\fboxrule{1pt} % \fbox{}
\usepackage{wrapfig} % 
\newcommand{\prob}{\mathbb{P}}
\newcommand{\norma}{\mathscr{N}}
\newcommand{\expect}{\mathbb{E}}
\newcommand{\summa}{\sum_{i=1}^n}
\newcommand{\yrseduc}{\textit{yrseduc}}
\usepackage[left=7mm, top=20mm, right=15mm, bottom=20mm, nohead, footskip=10mm]{geometry} % 
\usepackage{tikz} % 
\def\myrad{2cm}% radius of the circle
\def\myanga{45}% angle for the arc
\def\myangb{195}
\begin{document} % 
	\begin{flushright}
	\begin{tabular}{r}
		Danil Fedchenko, MAE 2020, group A \\
	\end{tabular}
\end{flushright}


\begin{center}
	Econometrics 1. Problem Set 2.
\end{center}
\section*{Problem 1}
Suppose that the population model determining $y$ is
\begin{align*}
y = \beta_0 + \beta_1 x_1 + \beta_2x_2 + \beta_3x_3 + u
\end{align*}
and this model satisfies the Gauss-Markov assumptions. However, we estimate the model that
omits $x_3$. Let $\tilde{\beta_1}, \tilde{\beta_2}$ be the OLS estimators from the regression of $y$ on $x_1$ and $x_2$. Show
that the expected value of $\tilde{\beta_1}$ (given the values of the independent variables in the sample) is
\begin{align*}
\expect \tilde{\beta_1} = \beta_1 + \beta_3 \frac{\sum_{i=1}^n \hat{r_{i1}x_{i3}}}{\sum_{i=1}^n \hat{r_{i1}}^2}
\end{align*}
where the $\hat{r_{i1}}$ are the OLS residuals from the regression of $x_1$ on $x_2$.



\textbf{Solution}
Plugging $y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+\beta_3x_{i3}$ into
\begin{align*}
\tilde{\beta_1} = \frac{\sum_{i=1}^n \hat{r_{i1}}y_i}{\sum_{i=1}^n \hat{r_{i1}}^2}
\end{align*}
one can obtain:
\begin{align} \label{eq1}
\tilde{\beta_1} = \frac{\sum_{i=1}^n \hat{r_{i1}}(\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+\beta_3x_{i3} + u_i)}{\sum_{i=1}^n \hat{r_{i1}}^2} = \nonumber \\
=\frac{\beta_0 \sum_{i=1}^n \hat{r_{i1}} + \beta_1 \sum_{i=1}^n \hat{r_{i1}}x_{i1} + \beta_2 \sum_{i=1}^n \hat{r_{i1}}x_{i2} + \beta_2 \sum_{i=1}^n \hat{r_{i1}}x_{i2} + \sum_{i=1}^n \hat{r_{i1}}u_i}{\sum_{i=1}^n \hat{r_{i1}}^2}
\end{align}
By properties of OLS estimators (corollary from FOC): $\sum_{i=1}^n \hat{r_{i1}} = 0, \sum_{i=1}^n \hat{r_{i1}}x_{i2} = 0$. Moreover,
\begin{align*}
\sum_{i=1}^n \hat{r_{i1}}x_{i1} = \sum_{i=1}^n \hat{r_{i1}} (\hat{\alpha_0} + \hat{\alpha_1}x_{i2} + \hat{r_{i1}}) = 0 + 0 + \sum_{i=1}^n \hat{r_{i1}}^2
\end{align*}
where $\hat{\alpha_0}, \hat{\alpha_1}$ are OLS estimates for coefficients of regression of $x_1$ on $x_2$. Plugging it into (\ref{eq1}):
\begin{align*}
\tilde{\beta_1} = \beta_1 + \beta_3\frac{\sum_{i=1}^n\hat{r_{i1}}x_{i3}}{\sum_{i=1}^n \hat{r_{i1}}^2} + \frac{\sum_{i=1}^n \hat{r_{i1}}u_i}{\sum_{i=1}^n \hat{r_{i1}}^2}\\
\expect \tilde{\beta_1} = \beta_1 + \beta_3\frac{\sum_{i=1}^n\hat{r_{i1}}x_{i3}}{\sum_{i=1}^n \hat{r_{i1}}^2} + \expect\left[ \frac{\sum_{i=1}^n \hat{r_{i1}}u_i}{\sum_{i=1}^n \hat{r_{i1}}^2} \right]
\end{align*}
\begin{align*}
\hat{r_{i1}} = x_{i1} - \hat{x_{i1}} = x_{i1} - \hat{\alpha_0} - \hat{\alpha_1}x_{i2}\ \to \expect[f(\hat{r_{i1}})|x_{i1}, x_{i2}] = f(\hat{r_{i1}}) \ \forall\ f(\cdot)
\end{align*}
Then,
\begin{align*}
\expect\left[\frac{\sum_{i=1}^n \hat{r_{i1}}u_i}{\sum_{i=1}^n \hat{r_{i1}}^2} \right] = \expect\left[\expect\left[\frac{\sum_{i=1}^n \hat{r_{i1}}u_i}{\sum_{i=1}^n \hat{r_{i1}}^2}\bigg|x_{1}, x_{2}\right] \right] \underset{\text{i.i.d.}}{\propto} \expect\left[\sum_{i=1}^n\hat{r_{i1}}\expect [u_i|x_{i1}, x_{i2}]\right] \underset{\text{G.-M. assump.}}{=} 0 
\end{align*}
Thus, 
\begin{align*}
\expect \tilde{\beta_1} = \beta_1 + \beta_3\frac{\sum_{i=1}^n\hat{r_{i1}}x_{i3}}{\sum_{i=1}^n \hat{r_{i1}}^2}
\end{align*}
\section*{Problem 2}
\begin{enumerate}[(i)]
	\item Consider the simple regression model $y = \beta_0 +\beta_1x+u$ under the first four Gauss-Markov
	assumptions. For some function $g(x)$, for example $g(x) = x^2$ or $g(x) = \ln (1 + x^2)$, define
	$z_i = g(x_i)$. Define a slope estimator as
	\begin{align*}
	\tilde{\beta_1} = \frac{\sum_{i=1}^n (z_i - \bar{z})y_i}{\sum_{i=1}^n (z_i - \bar{z})x_i}
	\end{align*}
	Show that $\tilde{\beta_1}$ is linear and unbiased.
	\item Add the homoskedasticity assumption, MLR.5. Show that
	\begin{align*}
	Var \tilde{\beta_1} = \sigma^2 \frac{\sum_{i=1}^n (z_i -  \bar{z})^2}{\left(\sum_{i=1}^n (z_i - \bar{z})x_i\right)^2}
	\end{align*}
	\item Show directly that, under the Gauss-Markov assumptions, $Var\hat{\beta_1} \le Var\tilde{\beta_1}$, where $\hat{\beta_1}$ is the OLS estimator.
\end{enumerate}



\textbf{Solution}

\begin{enumerate}[(i)]
	\item Since 
	\begin{align*}
	\tilde{\beta_1} = a_1 y_1 + \dots + a_ny_n
	\end{align*}
	is a linear function of $y_1, \dots, y_n$ with some coefficients, thus, it is a linear estimator. Now let us show unbiasedness  
	\begin{align*}
	\tilde{\beta_1} = \frac{\sum_{i=1}^n (z_i - \bar{z})(\beta_0 + \beta_1 x_i + u_i)}{\sum_{i=1}^n(z_i - \bar{z})x_i} = \beta_1 + \frac{\sum_{i=1}^n(z_i - \bar{z})u_i}{\sum_{i=1}^n(z_i - \bar{z})x_i}\\
	\expect \tilde{\beta_1} = \beta_1 + \expect \left[\frac{\sum_{i=1}^n(z_i - \bar{z})u_i}{\sum_{i=1}^n(z_i - \bar{z})x_i}\right]
	\end{align*}
	\begin{align*}
	\expect[(z_i - \bar{z})u_i] \underset{\expect u_i = 0}{ = } \expect[z_iu_i] = \expect[\expect[z_iu_i|x_1, \dots, x_n]] \underset{\text{i.i.d}}{ = } \expect[z_i\expect[u_i|x_i]] \underset{\text{G.-M. assump.}}{ = } 0 
	\end{align*}
	Hence 
	\begin{align*}
	\expect \tilde{\beta_1} = \beta_1
	\end{align*}
	i.e. the estimator is unbiased.
	\item Assume $Var(u|x) = \sigma^2$ then
	\begin{align*}
	Var \tilde{\beta_1} &= Var\left[\frac{\sum_{i=1}^n (z_i - \bar{z})y_i}{\sum_{i=1}^n (z_i - \bar{z})x_i}\right] = \frac{1}{\left(\sum_{i=1}^n (z_i - \bar{z})x_i\right)^2} Var\left[\sum_{i=1}^n (z_i - \bar{z})y_i\right] \underset{\text{i.i.d}}{=}\\
	&\underset{\text{i.i.d}}{=}\frac{1}{\left(\sum_{i=1}^n (z_i - \bar{z})x_i\right)^2} \sum_{i=1}^n Var((z_i - \bar{z})y_i)\\
	&Var((z_i - \bar{z})y_i) = \expect[Var[((z_i - \bar{z})y_i)|x_1, \dots, x_n]] + Var(\expect[((z_i - \bar{z})y_i)|x_1, \dots, x_n]) = \\
	&=\expect[(z_i - \bar{z})^2 Var(y_i|x_1, \dots, x_n)] + Var((z_i - \bar{z})\expect[y_i|x_1, \dots, x_n]) \underset{\text{i.i.d}}{=}\\
	&\underset{\text{i.i.d}}{=} \expect[(z_i-\bar{z})^2 Var(y_i|x_i)] + Var((z_i - \bar{z})\expect[y_i|x_i])
	\end{align*}
	\begin{align*}
	Var(y_i|x_i) = Var(\beta_0 + \beta_1x_i + u_i|x_i) = Var(u_i|x_i) = \sigma^2\\
	\expect[y_i|x_i] = \expect[\beta_0 + \beta_1x_i + u_i|x_i] = \beta_0 + \beta_1x_i
	\end{align*}
	Hence
	\begin{align*}
	Var ((z_i - \bar{z})y_i) = (z_i - \bar{z})^2\sigma^2 + Var(const) = (z_i - \bar{z})^2\sigma^2
	\end{align*}
	Thus, 
	\begin{align*}
	Var \tilde{\beta_1} = \sigma^2 \frac{\sum_{i=1}^n (z_i -  \bar{z})^2}{\left(\sum_{i=1}^n (z_i - \bar{z})x_i\right)^2}
	\end{align*}
	\item 
	\begin{align*}
	\hat{\beta_1} = \frac{\sum_{i=1}^n(x_i - \bar{x})y_i}{\sum_{i=1}^n(x_i - \bar{x})^2}
	\end{align*}
	Similarly to (ii):
	\begin{align*}
	Var \hat{\beta_1} = \frac{ \sigma^2}{\sum_{i=1}^n(x_i - \bar{x})^2}
	\end{align*}
	By the Cauchy - Schwartz inequality:
	\begin{align*}
	\left(\sum_{i=1}^n (z_i - \bar{z})(x_i - \bar{x})\right)^2 = \left(\sum_{i=1}^n (z_i - \bar{z})x_i\right)^2 \le \left(\sum_{i=1}^n (x_i - \bar{x})^2\right) \left(\sum_{i=1}^n (z_i - \bar{z})^2\right)
	\end{align*} 
	hence
	\begin{align*}
	Var \tilde{\beta_1} \ge \sigma^2 \frac{\summa (z_i - \bar{z})^2}{\left(\sum_{i=1}^n (x_i - \bar{x})^2\right) \left(\sum_{i=1}^n (z_i - \bar{z})^2\right)} = \frac{ \sigma^2}{\sum_{i=1}^n(x_i - \bar{x})^2} = Var \hat{\beta_1}
	\end{align*}
\end{enumerate}
\section*{Problem 3}
Consider the standard linear multivariate regression model
\begin{align*}
y = \beta_0 + \beta_1x_1 + \beta_2x_2 + u
\end{align*}
under the Gauss-Markov assumptions.
\begin{enumerate}[(a)]
\item Derive the OLS estimate $\hat{\beta_2}$ in terms of sample variances and sample covariances of
random variables $x_1$, $x_2$ and $y$.
\item How would you get the OLS estimates $\hat{\beta_1}$ and $\hat{\beta_2}$ by computing a series of OLS estimates
from simple regressions only?
\item Suppose you have decided to get OLS estimates for $\gamma_0$, $\gamma_1$ and $\gamma_2$ from
\begin{align*}
x_2 = \gamma_0 + \gamma_1x_1 + \gamma_2 y + \nu.
\end{align*}
Is $\tilde{\beta_2} = \frac{1}{\hat{\gamma_2}}$ an unbiased estimate of $\beta_2$?
\end{enumerate}


\textbf{Solution}


\begin{enumerate}[(a)]
	\item Denote sample variances as $s^2_{ab}, a, b \in \left\{y, x_1, x_2\right\}$. To compute the OLS estimate we need to solve the following optimization problem:
	\begin{align*}
	\underset{b_0, b_1, b_2}{\min} \summa (y_i - b_0 - b_1x_{1i} - b_2x_{2i})^2\\
	\text{FOCs:} \begin{cases}
	\summa (y_i - b_0 - b_1x_{1i} - b_2x_{2i}) = 0\\
	\summa (y_i - b_0 - b_1x_{1i} - b_2x_{2i})x_{1i} = 0\\
	\summa (y_i - b_0 - b_1x_{1i} - b_2x_{2i})x_{2i} = 0
	\end{cases}\\
	b_0 = \bar{y} - b_1\bar{x_1} - b_2 \bar{x_2}\\
	b_1\left(\summa x_{1i}^2\right) = \summa (y_i - \bar{y} + b_1\bar{x_1} + b_2\bar{x_2} - b_2x_{2i})x_{1i}
	\end{align*}
	\begin{align*}
	b_1 = \frac{\summa (y_i - \bar{y} - b_2(x_{2i} - \bar{x}))x_{1i}}{\summa (x_{1i} - \bar{x})^2}&=\frac{\summa (y_i - \bar{y})(x_{1i} - \bar{x_1}) - b_2\summa(x_{2i} - \bar{x_2})(x_{1i} - \bar{x_1})}{\summa (x_{1i} - \bar{x})^2} =\\
	&=\frac{s^2_{yx_1} - b_2s^2_{x_1x_2}}{s^2_{x_1x_1}}
	\end{align*}
	Similarly for $b_2$:
	\begin{align}\label{eq2}
	b_2 &= \frac{s^2_{yx_2} - b_1s^2_{x_1x_2}}{s^2_{x_2x_2}}\nonumber\\
	\text{hence }\nonumber\\
	b_2 &= \frac{s^2_{yx_2}s^2_{x_1x_1} - s^2_{yx_1}s^2_{x_2x_1}}{s^2_{x_1x_1}s^2_{x_2x_2} - (s^2_{x_1x_2})^2}
	\end{align}
	\item Performing simple linear regression of $y$ on $x_1$, $y$ on $x_2$, $x_1$ on $x_2$ and $x_2$ on $x_1$ denote OLS estimates for slope coefficients as $\alpha_1, \alpha_2, \alpha_3, \alpha_4$ respectively. It is known that
	\begin{align*}
	\alpha_1 = \frac{s^2_{yx_1}}{s^2_{x_1x_1}},\ \alpha_2 = \frac{s^2_{yx_2}}{s^2_{x_2x_2}},\ \alpha_3 = \frac{s^2_{x_1x_2}}{s^2_{x_2x_2}},\ \alpha_4 = \frac{s^2_{x_1x_2}}{s^2_{x_1x_1}}
	\end{align*}
	Divide numerator and denumerator of (\ref{eq2}) by $(s^2_{x_1x_1})^2$ (assuming that it is not equal to 0):
	\begin{align*}
	b_2 = \frac{\alpha_2\frac{\alpha_4}{\alpha_3} - \alpha_1\alpha_4}{\frac{\alpha_4}{\alpha_3} - \alpha_4^2} = \frac{\frac{\alpha_2}{\alpha_3} - \alpha_1}{\frac{1}{\alpha_3} - \alpha_4}
	\end{align*}
	Similarly for $b_1$:
	\begin{align*}
	b_1 = \frac{\alpha_1 - \alpha_2 \alpha_4}{1 - \alpha_3}
	\end{align*}
	\item From (\ref{eq2}):
	\begin{align*}
	\hat{\gamma_2} = \frac{s^2_{yx_2}s^2_{x_1x_1} - s^2_{x_2x_1}s^2_{yx_1}}{s^2_{x_1x_1}s^2_{yy} - (s^2_{x_1y})^2} \to \frac{1}{\hat{\gamma_2}} = \frac{s^2_{x_1x_1}s^2_{yy} - (s^2_{x_1y})^2}{s^2_{yx_2}s^2_{x_1x_1} - s^2_{x_2x_1}s^2_{yx_1}}\\
	\end{align*}
	To find $\expect[\frac{1}{\hat{\gamma_2}}]$ firstly observe the following facts:
	\begin{align*}
	s^2_{x_1x_1}s^2_{yy} = s^2_{x_1x_1}\cdot(\summa (y_i - \bar{y})(\beta_1x_{1i} + \beta_2x_{2i} + u_i - \beta_1\bar{x_1} - \beta_2 \bar{x_2} - \bar{u})) = \\
	=s^2_{x_1x_1} (\beta_1s^2_{yx_1} + \beta_2s^2_{yx_2} + s^2_{uy})\\
	(s_{x_1y}^2)^2 = s_{x_1y}^2 \cdot (\summa (x_{1i} - \bar{x})(\beta_1x_{1i} + \beta_2x_{2i} + u_i - \beta_1\bar{x_1} - \beta_2\bar{x_2} - \bar{u})) =\\ =s_{x_1y}^2 (\beta_1 s^2_{x_1x_1} + \beta_2s^2_{x_1x_2} + s^2_{x_1u})
	\end{align*}
	hence
	\begin{align*}
	s^2_{x_1x_1}s^2_{yy} - (s_{x_1y}^2)^2 = \beta_2(s^2_{x_1x_1}s^2_{yx_2} - s^2_{x_1y}s^2_{x_1x_2}) + s^2_{x_1x_1}s^2_{uy} - s^2_{x_1y}s^2_{x_1u}
	\end{align*}
	Thus,
	\begin{align*}
	\frac{1}{\hat{\gamma_2}} = \beta_2 + \frac{s^2_{x_1x_1}s^2_{uy} - s^2_{x_1y}s^2_{x_1u}}{s^2_{x_1x_1}s^2_{yx_2} - s^2_{x_1y}s^2_{x_1x_2}}
	\end{align*}
\end{enumerate}
\section*{Problem 4}
Consider a simple regression model through the origin under the Gauss-Markov assumptions
\begin{align*}
y = \gamma x + \varepsilon
\end{align*}
and let
\begin{align*}
\tilde{\gamma} = \frac{\summa y_ix_i^3}{\summa x_i^4}
\end{align*}
be an estimator of the slope parameter $\gamma$.
\begin{enumerate}[(a)]
\item Under what condition this estimator is well defined?
\item Show conditional unbiasness of $\tilde{\gamma}$.
\item Derive conditional variance of the estimator.
\end{enumerate}


\textbf{Solution}


\begin{enumerate}[(a)]
	\item First of all $x_i \neq 0\ \forall i$ then the estimator is well-defined.
	\item \begin{align*}
	\expect[\tilde{\gamma}|x_1, \dots, x_n] = \expect\left[\frac{\summa (\gamma x_i + \varepsilon_i)x_i^3}{\summa x_i^4}\bigg|x_1, \dots, x_n\right] &= \gamma + \frac{1}{\summa x_i^4} \summa x_i^3\expect[\varepsilon_i|x_1, \dots, x_n] =\\
	 &\underset{\text{G.-M. assump}}{=} \gamma + 0
	\end{align*}
	\item \begin{align*}
	Var(\tilde{\gamma}|x_1, \dots, x_n) = Var\left[\frac{\summa (\gamma x_i + \varepsilon_i)x_i^3}{\summa x_i^4}\bigg|x_1, \dots, x_n\right] \underset{\text{i.i.d}}{=} \frac{\summa x_i^6 Var[\varepsilon_i|x_i]}{\left(\summa x_i^4\right)^2} 
	\end{align*}
	If $Var(\varepsilon|x) = const = \sigma^2$ then
	\begin{align*}
	Var(\tilde{\gamma}|x_1, \dots, x_2) = \sigma^2 \frac{\summa x_i^6}{\left(\summa x_i^4\right)^2} 
	\end{align*}
\end{enumerate}
\section*{Problem 5}
In this problem set you continue to explore the returns to education and the gender gap
in earnings. The data set is the same as for problem set 1 (cps99 ps1.csv; see problem set 1
for a description of the variables). Consider the regression of average hourly earnings (\textit{ahe}) on
years of education (\textit{yrseduc}). Consider the following three variables that have been omitted
from the regression:
\begin{enumerate}[(a)]
\item gender
\item a binary variable that = 1 if the worker’s last name falls in the first half of the alphabet,
and = 0 if it falls in the second half.
\item the worker’s native ability (for example IQ or some better measure)
\end{enumerate}
For each: Will this omission arguably lead to omitted variable bias? Why or why not?
If your answer is that it will, state the sign of



\textbf{Solution}

Omitted variable bias appears when omitted variable is correlated with explanatory variables and affects dependent variable.
\begin{enumerate}[(a)]
	\item First of all, let us compute a sample correlation between gender and \textit{yrseduc}. The correlation is insignificantly small, hence we have reason to believe that gender is actualy not correlated with \textit{yrseduc}. Indeed, in most of countries there is no an arguable relationship between gender of the person and number of his or her years of education. However, in Russia, for example, and maybe in other countries with a conscript army system (except Israel, I think) I think men usually tend to spend more years being enrolled in some educational institutions in order not to go to the army. So, in general omission of gender variable can indeed lead to the omitted variable bias, but in our particular case, I suppose it is not a problem.
	\item I think that such a variable both does not affect the response variable and is not correlated with years of education, so there is no omitted variable bias.
	\item Yes, the omission of the worker's naitive ability can lead to the omitted variable bias. Because I suppose it both affects the average salary and is correlated with number of years of education. In general, I think the bias should be positive i.e. obtained in the misspecified model OLS estimate of the slope coefficient sould be on average greater than the true value of slope coefficient for \textit{yrseduc}. Since $\tilde{\beta_1} = \hat{\beta_1} + \hat{\beta_2} \delta$ where $\tilde{\beta_1}$ is the OLS estimate of the coefficient of misspecified model, $\hat{\beta_1}, \hat{\beta_2}$ are OLS estimates for the miltiple regression with both variables \textit{yrseduc} and innate abilities, and $\delta$ is the OLS estimate in regression innate abilities on \textit{yrseduc}. It can be reasonable to assume that the more skillful pearson is the more higher his salary is, hence $\hat{\beta_2} > 0$. Not as much reasonable would it assume that the more skillful pearson is the more years he spend on education, hence $\delta > 0$. However it can be the case that more skillful people on the contrary spend less years on education (e.g. Bill Gates, Steave Jobs) because of high opportunity costs of it, in this situation $\delta < 0$ and bias is negative.
\end{enumerate}
\section*{Problem 6}
Consider an experimental approach to estimation of the effect on earnings of education.
\begin{enumerate}[(a)]
\item Describe an experimental protocol for an idealized randomized controlled experiment
that would permit unbiased estimation of the effect on earnings of an additional year of education at a typical US educational institution. (Ignore practical and ethical issues.)
\item Explain why, precisely, your experimental protocol would eliminate omitted variable
bias arising from omission of the variables in question 1 (or any other variables).
\end{enumerate}



\textbf{Solution}
\begin{enumerate}[(a)]
	\item An idealized randomized controlled experiment should be as follows: we should absolutely randomly choose two groups of people. Absolutely randomly in the sense that each person of the USA could equally likely be chosen for this experiment. People of the first group should be given an additional year of education, while people from the second group should be abstained from the education over the year. Then after this year 
\end{enumerate}
\section*{Problem 7}
Estimate two regressions: (i) \textit{ahe} on \textit{yrseduc} and (ii) \textit{ahe} on \textit{yrseduc} and \textit{female}.
\begin{enumerate}[(a)]
\item In regression (ii), what is the coefficient on \yrseduc? Explain what this means.
\item In regression (ii), test the hypothesis that the population coefficient on female in
specification (ii) is zero, against the hypothesis that it is nonzero, at the 5\% significance level.
In everyday words (not statistical terms), what precisely is the hypothesis is that you are
testing?
\item Does the coefficient on \textit{yrseduc} change from regressions (i) to (ii) in a substantively
important way, that is, is the difference between the two estimates large in a real-world sense?
What is the reason that including \textit{female} in the regression results in a large (or only a small)
change in the coefficient on \yrseduc? (Hint: what is the correlation between \yrseduc\ and
\textit{female}?)
\end{enumerate}


\textbf{Solution}

\begin{enumerate}[(a)]
	\item Coefficient on \yrseduc\ is equal to 1.34147. That means that holding a gender of the person fixed each additional year of education on average gives 1.34147 dollars of increase of the per hour earnings.
	\item Using coeftest() it can be obtained that the null hypothesis should be rejected on the 5\% significance level. The hypothesis was that holding a number of years of education fixed both males and females get on average equal salary per hour.
	\item Coefficient on \yrseduc\ did not significantly change: from 1.32 to 1.34. The reason is the follwoing that as it has been already indicated in the problem 5 (a) the sample correlation between \yrseduc\ and \textit{female} is very low, as a result, using formula from the 5 (c) $\tilde{\beta_1} = \hat{\beta_1} + \hat{\beta_2} \delta$ one can conclude that extremely small $\delta$ results in $\tilde{\beta_1} \approx \hat{\beta_1}$ even if $\hat{\beta_2} \neq 0$.
\end{enumerate}
\end{document}