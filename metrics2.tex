\documentclass[a4paper]{article}
\usepackage[14pt]{extsizes} % 
\usepackage[utf8]{inputenc}
\usepackage{setspace,amsmath}
\usepackage{mathtools}
\usepackage{pgfplots}
\usepackage{titlesec}
\usepackage{pdfpages}
\usepackage[shortlabels]{enumitem}
\usepackage{tikz}
\usetikzlibrary{angles,quotes}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{float}
\usepackage[section]{placeins}
\usepackage[makeroom]{cancel}
\usepackage{mathrsfs} % 
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
%\addto\captionsrussian{\renewcommand{\figurename}{Fig.}}
\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools} 
\newcommand*{\hm}[1]{#1\nobreak\discretionary{}
{\hbox{$\mathsurround=0pt #1$}}{}}
\usepackage{graphicx}  % 
\graphicspath{{images/}{images2/}}  % 
\setlength\fboxsep{3pt} %  \fbox{} 
\setlength\fboxrule{1pt} % \fbox{}
\usepackage{wrapfig} % 
\newcommand{\prob}{\mathbb{P}}
\newcommand{\norma}{\mathscr{N}}
\newcommand{\expect}{\mathbb{E}}
\newcommand{\summa}{\sum_{i=1}^n}
\usepackage[left=7mm, top=20mm, right=15mm, bottom=20mm, nohead, footskip=10mm]{geometry} % 
\usepackage{tikz} % 
\def\myrad{2cm}% radius of the circle
\def\myanga{45}% angle for the arc
\def\myangb{195}
\begin{document} % 
	\begin{flushright}
	\begin{tabular}{r}
		Danil Fedchenko, MAE 2020, group A \\
	\end{tabular}
\end{flushright}


\begin{center}
	Econometrics 1. Problem Set 2.
\end{center}
\section*{Problem 1}
Suppose that the population model determining $y$ is
\begin{align*}
y = \beta_0 + \beta_1 x_1 + \beta_2x_2 + \beta_3x_3 + u
\end{align*}
and this model satisfies the Gauss-Markov assumptions. However, we estimate the model that
omits $x_3$. Let $\tilde{\beta_1}, \tilde{\beta_2}$ be the OLS estimators from the regression of $y$ on $x_1$ and $x_2$. Show
that the expected value of $\tilde{\beta_1}$ (given the values of the independent variables in the sample) is
\begin{align*}
\expect \tilde{\beta_1} = \beta_1 + \beta_3 \frac{\sum_{i=1}^n \hat{r_{i1}x_{i3}}}{\sum_{i=1}^n \hat{r_{i1}}^2}
\end{align*}
where the $\hat{r_{i1}}$ are the OLS residuals from the regression of $x_1$ on $x_2$.



\textbf{Solution}
Plugging $y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+\beta_3x_{i3}$ into
\begin{align*}
\tilde{\beta_1} = \frac{\sum_{i=1}^n \hat{r_{i1}}y_i}{\sum_{i=1}^n \hat{r_{i1}}^2}
\end{align*}
one can obtain:
\begin{align} \label{eq1}
\tilde{\beta_1} = \frac{\sum_{i=1}^n \hat{r_{i1}}(\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+\beta_3x_{i3} + u_i)}{\sum_{i=1}^n \hat{r_{i1}}^2} = \nonumber \\
=\frac{\beta_0 \sum_{i=1}^n \hat{r_{i1}} + \beta_1 \sum_{i=1}^n \hat{r_{i1}}x_{i1} + \beta_2 \sum_{i=1}^n \hat{r_{i1}}x_{i2} + \beta_2 \sum_{i=1}^n \hat{r_{i1}}x_{i2} + \sum_{i=1}^n \hat{r_{i1}}u_i}{\sum_{i=1}^n \hat{r_{i1}}^2}
\end{align}
By properties of OLS estimators (corollary from FOC): $\sum_{i=1}^n \hat{r_{i1}} = 0, \sum_{i=1}^n \hat{r_{i1}}x_{i2} = 0$. Moreover,
\begin{align*}
\sum_{i=1}^n \hat{r_{i1}}x_{i1} = \sum_{i=1}^n \hat{r_{i1}} (\hat{\alpha_0} + \hat{\alpha_1}x_{i2} + \hat{r_{i1}}) = 0 + 0 + \sum_{i=1}^n \hat{r_{i1}}^2
\end{align*}
where $\hat{\alpha_0}, \hat{\alpha_1}$ are OLS estimates for coefficients of regression of $x_1$ on $x_2$. Plugging it into (\ref{eq1}):
\begin{align*}
\tilde{\beta_1} = \beta_1 + \beta_3\frac{\sum_{i=1}^n\hat{r_{i1}}x_{i3}}{\sum_{i=1}^n \hat{r_{i1}}^2} + \frac{\sum_{i=1}^n \hat{r_{i1}}u_i}{\sum_{i=1}^n \hat{r_{i1}}^2}\\
\expect \tilde{\beta_1} = \beta_1 + \beta_3\frac{\sum_{i=1}^n\hat{r_{i1}}x_{i3}}{\sum_{i=1}^n \hat{r_{i1}}^2} + \expect\left[ \frac{\sum_{i=1}^n \hat{r_{i1}}u_i}{\sum_{i=1}^n \hat{r_{i1}}^2} \right]
\end{align*}
\begin{align*}
\hat{r_{i1}} = x_{i1} - \hat{x_{i1}} = x_{i1} - \hat{\alpha_0} - \hat{\alpha_1}x_{i2}\ \to \expect[f(\hat{r_{i1}})|x_{i1}, x_{i2}] = f(\hat{r_{i1}}) \ \forall\ f(\cdot)
\end{align*}
Then,
\begin{align*}
\expect\left[\frac{\sum_{i=1}^n \hat{r_{i1}}u_i}{\sum_{i=1}^n \hat{r_{i1}}^2} \right] = \expect\left[\expect\left[\frac{\sum_{i=1}^n \hat{r_{i1}}u_i}{\sum_{i=1}^n \hat{r_{i1}}^2}\bigg|x_{1}, x_{2}\right] \right] \underset{\text{i.i.d.}}{\propto} \expect\left[\sum_{i=1}^n\hat{r_{i1}}\expect [u_i|x_{i1}, x_{i2}]\right] \underset{\text{G.-M. assump.}}{=} 0 
\end{align*}
Thus, 
\begin{align*}
\expect \tilde{\beta_1} = \beta_1 + \beta_3\frac{\sum_{i=1}^n\hat{r_{i1}}x_{i3}}{\sum_{i=1}^n \hat{r_{i1}}^2}
\end{align*}
\section*{Problem 2}
\begin{enumerate}[(i)]
	\item Consider the simple regression model $y = \beta_0 +\beta_1x+u$ under the first four Gauss-Markov
	assumptions. For some function $g(x)$, for example $g(x) = x^2$ or $g(x) = \ln (1 + x^2)$, define
	$z_i = g(x_i)$. Define a slope estimator as
	\begin{align*}
	\tilde{\beta_1} = \frac{\sum_{i=1}^n (z_i - \bar{z})y_i}{\sum_{i=1}^n (z_i - \bar{z})x_i}
	\end{align*}
	Show that $\tilde{\beta_1}$ is linear and unbiased.
	\item Add the homoskedasticity assumption, MLR.5. Show that
	\begin{align*}
	Var \tilde{\beta_1} = \sigma^2 \frac{\sum_{i=1}^n (z_i -  \bar{z})^2}{\left(\sum_{i=1}^n (z_i - \bar{z})x_i\right)^2}
	\end{align*}
	\item Show directly that, under the Gauss-Markov assumptions, $Var\hat{\beta_1} \le Var\tilde{\beta_1}$, where $\hat{\beta_1}$ is the OLS estimator.
\end{enumerate}



\textbf{Solution}

\begin{enumerate}[(i)]
	\item Since 
	\begin{align*}
	\tilde{\beta_1} = a_1 y_1 + \dots + a_ny_n
	\end{align*}
	is a linear function of $y_1, \dots, y_n$ with some coefficients, thus, it is a linear estimator. Now let us show unbiasedness  
	\begin{align*}
	\tilde{\beta_1} = \frac{\sum_{i=1}^n (z_i - \bar{z})(\beta_0 + \beta_1 x_i + u_i)}{\sum_{i=1}^n(z_i - \bar{z})x_i} = \beta_1 + \frac{\sum_{i=1}^n(z_i - \bar{z})u_i}{\sum_{i=1}^n(z_i - \bar{z})x_i}\\
	\expect \tilde{\beta_1} = \beta_1 + \expect \left[\frac{\sum_{i=1}^n(z_i - \bar{z})u_i}{\sum_{i=1}^n(z_i - \bar{z})x_i}\right]
	\end{align*}
	\begin{align*}
	\expect[(z_i - \bar{z})u_i] \underset{\expect u_i = 0}{ = } \expect[z_iu_i] = \expect[\expect[z_iu_i|x_1, \dots, x_n]] \underset{\text{i.i.d}}{ = } \expect[z_i\expect[u_i|x_i]] \underset{\text{G.-M. assump.}}{ = } 0 
	\end{align*}
	Hence 
	\begin{align*}
	\expect \tilde{\beta_1} = \beta_1
	\end{align*}
	i.e. the estimator is unbiased.
	\item Assume $Var(u|x) = \sigma^2$ then
	\begin{align*}
	Var \tilde{\beta_1} &= Var\left[\frac{\sum_{i=1}^n (z_i - \bar{z})y_i}{\sum_{i=1}^n (z_i - \bar{z})x_i}\right] = \frac{1}{\left(\sum_{i=1}^n (z_i - \bar{z})x_i\right)^2} Var\left[\sum_{i=1}^n (z_i - \bar{z})y_i\right] \underset{\text{i.i.d}}{=}\\
	&\underset{\text{i.i.d}}{=}\frac{1}{\left(\sum_{i=1}^n (z_i - \bar{z})x_i\right)^2} \sum_{i=1}^n Var((z_i - \bar{z})y_i)\\
	&Var((z_i - \bar{z})y_i) = \expect[Var[((z_i - \bar{z})y_i)|x_1, \dots, x_n]] + Var(\expect[((z_i - \bar{z})y_i)|x_1, \dots, x_n]) = \\
	&=\expect[(z_i - \bar{z})^2 Var(y_i|x_1, \dots, x_n)] + Var((z_i - \bar{z})\expect[y_i|x_1, \dots, x_n]) \underset{\text{i.i.d}}{=}\\
	&\underset{\text{i.i.d}}{=} \expect[(z_i-\bar{z})^2 Var(y_i|x_i)] + Var((z_i - \bar{z})\expect[y_i|x_i])
	\end{align*}
	\begin{align*}
	Var(y_i|x_i) = Var(\beta_0 + \beta_1x_i + u_i|x_i) = Var(u_i|x_i) = \sigma^2\\
	\expect[y_i|x_i] = \expect[\beta_0 + \beta_1x_i + u_i|x_i] = \beta_0 + \beta_1x_i
	\end{align*}
	Hence
	\begin{align*}
	Var ((z_i - \bar{z})y_i) = (z_i - \bar{z})^2\sigma^2 + Var(const) = (z_i - \bar{z})^2\sigma^2
	\end{align*}
	Thus, 
	\begin{align*}
	Var \tilde{\beta_1} = \sigma^2 \frac{\sum_{i=1}^n (z_i -  \bar{z})^2}{\left(\sum_{i=1}^n (z_i - \bar{z})x_i\right)^2}
	\end{align*}
	\item 
	\begin{align*}
	\hat{\beta_1} = \frac{\sum_{i=1}^n(x_i - \bar{x})y_i}{\sum_{i=1}^n(x_i - \bar{x})^2}
	\end{align*}
	Similarly to (ii):
	\begin{align*}
	Var \hat{\beta_1} = \frac{ \sigma^2}{\sum_{i=1}^n(x_i - \bar{x})^2}
	\end{align*}
	By the Cauchy - Schwartz inequality:
	\begin{align*}
	\left(\sum_{i=1}^n (z_i - \bar{z})(x_i - \bar{x})\right)^2 = \left(\sum_{i=1}^n (z_i - \bar{z})x_i\right)^2 \le \left(\sum_{i=1}^n (x_i - \bar{x})^2\right) \left(\sum_{i=1}^n (z_i - \bar{z})^2\right)
	\end{align*} 
	hence
	\begin{align*}
	Var \tilde{\beta_1} \ge \sigma^2 \frac{\summa (z_i - \bar{z})^2}{\left(\sum_{i=1}^n (x_i - \bar{x})^2\right) \left(\sum_{i=1}^n (z_i - \bar{z})^2\right)} = \frac{ \sigma^2}{\sum_{i=1}^n(x_i - \bar{x})^2} = Var \hat{\beta_1}
	\end{align*}
\end{enumerate}
\section*{Problem 3}
Consider the standard linear multivariate regression model
\begin{align*}
y = \beta_0 + \beta_1x_1 + \beta_2x_2 + u
\end{align*}
under the Gauss-Markov assumptions.
\begin{enumerate}[(a)]
\item Derive the OLS estimate $\hat{\beta_2}$ in terms of sample variances and sample covariances of
random variables $x_1$, $x_2$ and $y$.
\item How would you get the OLS estimates $\hat{\beta_1}$ and $\hat{\beta_2}$ by computing a series of OLS estimates
from simple regressions only?
\item Suppose you have decided to get OLS estimates for $\gamma_0$, $\gamma_1$ and $\gamma_2$ from
\begin{align*}
x_2 = \gamma_0 + \gamma_1x_1 + \gamma_2 y + \nu.
\end{align*}
Is $\tilde{\beta_2} = \frac{1}{\hat{\gamma_2}}$ an unbiased estimate of $\beta_2$?
\end{enumerate}


\textbf{Solution}


\begin{enumerate}[(a)]
	\item Denote sample variances as $s^2_{ab}, a, b \in \left\{y, x_1, x_2\right\}$. To compute the OLS estimate we need to solve the following optimization problem:
	\begin{align*}
	\underset{b_0, b_1, b_2}{\min} \summa (y_i - b_0 - b_1x_{1i} - b_2x_{2i})^2\\
	\text{FOCs:} \begin{cases}
	\summa (y_i - b_0 - b_1x_{1i} - b_2x_{2i}) = 0\\
	\summa (y_i - b_0 - b_1x_{1i} - b_2x_{2i})x_{1i} = 0\\
	\summa (y_i - b_0 - b_1x_{1i} - b_2x_{2i})x_{2i} = 0
	\end{cases}\\
	b_0 = \bar{y} - b_1\bar{x_1} - b_2 \bar{x_2}\\
	b_1\left(\summa x_{1i}^2\right) = \summa (y_i - \bar{y} + b_1\bar{x_1} + b_2\bar{x_2} - b_2x_{2i})x_{1i}
	\end{align*}
	\begin{align*}
	b_1 = \frac{\summa (y_i - \bar{y} - b_2(x_{2i} - \bar{x}))x_{1i}}{\summa (x_{1i} - \bar{x})^2}&=\frac{\summa (y_i - \bar{y})(x_{1i} - \bar{x_1}) - b_2\summa(x_{2i} - \bar{x_2})(x_{1i} - \bar{x_1})}{\summa (x_{1i} - \bar{x})^2} =\\
	&=\frac{s^2_{yx_1} - b_2s^2_{x_1x_2}}{s^2_{x_1x_1}}
	\end{align*}
	Similarly for $b_2$:
	\begin{align}\label{eq2}
	b_2 = \frac{s^2_{yx_2} - b_1s^2_{x_1x_2}}{s^2_{x_2x_2}} &= \frac{s^2_{yx_2} - s^2_{yx_1} + b_2s^2_{x_1x_2}}{s^2_{x_2x_2}} \nonumber\\
	b_2 &= \frac{s^2_{yx_2} - s^2_{yx_1}}{s^2_{x_2x_2} - s^2_{x_1x_2}}
	\end{align}
	\item Performing simple linear regression of $y$ on $x_1$, $y$ on $x_2$, $x_1$ on $x_2$ and $x_2$ on $x_1$ denote OLS estimates for slope coefficients as $\alpha_1, \alpha_2, \alpha_3, \alpha_4$ respectively. It is known that
	\begin{align*}
	\alpha_1 = \frac{s^2_{yx_1}}{s^2_{x_1x_1}},\ \alpha_2 = \frac{s^2_{yx_2}}{s^2_{x_2x_2}},\ \alpha_3 = \frac{s^2_{x_1x_2}}{s^2_{x_2x_2}},\ \alpha_4 = \frac{s^2_{x_1x_2}}{s^2_{x_1x_1}}
	\end{align*}
	Divide numerator and denumerator of (\ref{eq2}) by $s^2_{x_1x_1}$ (assuming that it is not equal to 0):
	\begin{align*}
	b_2 = \frac{\alpha_2\frac{\alpha_4}{\alpha_3} - \alpha_1}{\frac{\alpha_4}{\alpha_3} - \alpha_4}
	\end{align*}
	Similarly for $b_1$:
	\begin{align*}
	b_1 = \frac{\alpha_1\frac{\alpha_3}{\alpha_4} - \alpha_2}{\frac{\alpha_3}{\alpha_4} - \alpha_3}
	\end{align*}
	\item From (\ref{eq2}):
	\begin{align*}
	\hat{\gamma_2} = \frac{s^2_{yx_2} - s^2_{x_1x_2}}{s^2_{yy} - s^2_{yx_1}} \to \frac{1}{\hat{\gamma_2}} = \frac{s^2_{yy} - s^2_{yx_1}}{s^2_{yx_2} - s^2_{x_1x_2}}
	\end{align*}
\end{enumerate}
\section*{Problem 4}
Consider a simple regression model through the origin under the Gauss-Markov assumptions
\begin{align*}
y = \gamma x + \varepsilon
\end{align*}
and let
\begin{align*}
\tilde{\gamma} = \frac{\summa y_ix_i^3}{\summa x_i^4}
\end{align*}
be an estimator of the slope parameter $\gamma$.
\begin{enumerate}[(a)]
\item Under what condition this estimator is well defined?
\item Show conditional unbiasness of $\tilde{\gamma}$.
\item Derive conditional variance of the estimator.
\end{enumerate}


\textbf{Solution}


\begin{enumerate}[(a)]
	\item First of all $x_i \neq 0\ \forall i$ then the estimator is well-defined.
	\item \begin{align*}
	\expect[\tilde{\gamma}|x_1, \dots, x_n] = \expect\left[\frac{\summa (\gamma x_i + \varepsilon_i)x_i^3}{\summa x_i^4}\bigg|x_1, \dots, x_n\right] &= \gamma + \frac{1}{\summa x_i^4} \summa x_i^3\expect[\varepsilon_i|x_1, \dots, x_n] =\\
	 &\underset{\text{G.-M. assump}}{=} \gamma + 0
	\end{align*}
	\item \begin{align*}
	Var(\tilde{\gamma}|x_1, \dots, x_n) = Var\left[\frac{\summa (\gamma x_i + \varepsilon_i)x_i^3}{\summa x_i^4}\bigg|x_1, \dots, x_n\right] \underset{\text{i.i.d}}{=} \frac{\summa x_i^6 Var[\varepsilon_i|x_i]}{\left(\summa x_i^4\right)^2} 
	\end{align*}
	If $Var(\varepsilon|x) = const = \sigma^2$ then
	\begin{align*}
	Var(\tilde{\gamma}|x_1, \dots, x_2) = \sigma^2 \frac{\summa x_i^6}{\left(\summa x_i^4\right)^2} 
	\end{align*}
\end{enumerate}
\end{document}