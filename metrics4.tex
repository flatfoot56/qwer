\documentclass[a4paper]{article}
\usepackage[14pt]{extsizes} % 
\usepackage[utf8]{inputenc}
\usepackage{setspace,amsmath}
\usepackage{mathtools}
\usepackage{pgfplots}
\usepackage{titlesec}
\usepackage{pdfpages}
\usepackage[shortlabels]{enumitem}
\usepackage{tikz}
\usetikzlibrary{angles,quotes}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{float}
\usepackage[section]{placeins}
\usepackage[makeroom]{cancel}
\usepackage{mathrsfs} % 
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
%\addto\captionsrussian{\renewcommand{\figurename}{Fig.}}
\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools} 
\newcommand*{\hm}[1]{#1\nobreak\discretionary{}
{\hbox{$\mathsurround=0pt #1$}}{}}
\usepackage{graphicx}  % 
\graphicspath{{images/}{images2/}}  % 
\setlength\fboxsep{3pt} %  \fbox{} 
\setlength\fboxrule{1pt} % \fbox{}
\usepackage{wrapfig} % 
\newcommand{\prob}{\mathbb{P}}
\newcommand{\norma}{\mathscr{N}}
\newcommand{\expect}{\mathbb{E}}
\newcommand{\summa}{\sum_{i=1}^n}
\newcommand{\yrseduc}{\textit{yrseduc}}
\usepackage[left=7mm, top=20mm, right=15mm, bottom=20mm, nohead, footskip=10mm]{geometry} % 
\usepackage{tikz} % 
\def\myrad{2cm}% radius of the circle
\def\myanga{45}% angle for the arc
\def\myangb{195}
\begin{document} % 
	\begin{flushright}
	\begin{tabular}{r}
		Danil Fedchenko, MAE 2020, group A \\
	\end{tabular}
\end{flushright}


\begin{center}
	Econometrics 1. Problem Set 4.
\end{center}
\section*{Problem 1}

\textbf{Solution}


Since $\hat{\beta_0}, \dots, \hat{\beta_k}$ are OLS estimates they satisfy the following system of equations:
\begin{align}\label{eq1}
\begin{cases}
\summa (y_i - \hat{\beta_0} - \hat{\beta_1}x_{1i} - \dots - \hat{\beta_k}x_{ki}) = 0\\
\summa x_{1i}(y_i - \hat{\beta_0} - \hat{\beta_1}x_{1i} - \dots - \hat{\beta_k}x_{ki}) = 0\\
\dots\\
\dots\\
\summa x_{ki}(y_i - \hat{\beta_0} - \hat{\beta_1}x_{1i} - \dots - \hat{\beta_k}x_{ki}) = 0
\end{cases}
\end{align}

This is also true for $\tilde{\beta_0}, \dots, \tilde{\beta_k}$, i.e.
\begin{align*}
\begin{cases}
\summa c_0(c_0y_i - \tilde{\beta_0} - \tilde{\beta_1}c_1x_{1i} - \dots - \tilde{\beta_k}c_kx_{ki}) = 0\\
\summa c_1x_{1i}(c_0y_i - \tilde{\beta_0} - \tilde{\beta_1}c_1x_{1i} - \dots - \tilde{\beta_k}c_kx_{ki}) = 0\\
\dots\\
\dots\\
\summa x_{ki}c_k(c_0y_i - \tilde{\beta_0} - \tilde{\beta_1}c_1x_{1i} - \dots - \tilde{\beta_k}c_kx_{ki}) = 0
\end{cases}
\end{align*}
Assuming that $c_0$ is not equal to 0 either, this system can be rewritten as follows:
\begin{align*}
\begin{cases}
\summa c_0^2(y_i - \frac{1}{c_0}\tilde{\beta_0} - \frac{c_1}{c_0}\tilde{\beta_1}x_{1i} - \dots - \frac{c_k}{c_0}\tilde{\beta_k}x_{ki}) = 0\\
\summa \frac{c_1}{c_0}x_{1i}(y_i - \frac{1}{c_0}\tilde{\beta_0} - \frac{c_1}{c_0}\tilde{\beta_1}x_{1i} - \dots - \frac{c_k}{c_0}\tilde{\beta_k}x_{ki}) = 0\\
\dots\\
\dots\\
\summa \frac{c_k}{c_0}x_{ki}(y_i - \frac{1}{c_0}\tilde{\beta_0} - \frac{c_1}{c_0}\tilde{\beta_1}x_{1i} - \dots - \frac{c_k}{c_0}\tilde{\beta_k}x_{ki}) = 0
\end{cases}
\end{align*}
or, finally
\begin{align*}
\begin{cases}
\summa (y_i - \frac{1}{c_0}\tilde{\beta_0} - \frac{c_1}{c_0}\tilde{\beta_1}x_{1i} - \dots - \frac{c_k}{c_0}\tilde{\beta_k}x_{ki}) = 0\\
\summa x_{1i}(y_i - \frac{1}{c_0}\tilde{\beta_0} - \frac{c_1}{c_0}\tilde{\beta_1}x_{1i} - \dots - \frac{c_k}{c_0}\tilde{\beta_k}x_{ki}) = 0\\
\dots\\
\dots\\
\summa x_{ki}(y_i - \frac{1}{c_0}\tilde{\beta_0} - \frac{c_1}{c_0}\tilde{\beta_1}x_{1i} - \dots - \frac{c_k}{c_0}\tilde{\beta_k}x_{ki}) = 0
\end{cases}
\end{align*}
Substituting 
\begin{align}\label{eq2}
\tilde{\beta_i} = \frac{c_0}{c_i}\tilde{\beta_i}', \forall\ i = 1, \dots, k\\
\tilde{\beta_0} = c_0 \tilde{\beta_0}' \nonumber
\end{align} 
we can get the system which is equivalent to the system \eqref{eq1}, as a result has the same solutions. Plugging these solutions $\hat{\beta_i}$ back to \eqref{eq2} we finally get
\begin{align*}
\tilde{\beta_i} = \frac{c_0}{c_i}\hat{\beta_i}\ \forall\ i=1, \dots, k\\
\tilde{\beta_0} = c_0 \hat{\beta_0}
\end{align*}
Q.E.D.

\section*{Problem 2}
\textbf{Solution}


\begin{align*}
\ln(wage) = \beta_0 + \beta_1 educ + \beta_2 educ \times pareduc + \beta_3 exper + \beta_4 tenure + u
\end{align*}

\begin{enumerate}[(i)]
	\item Obviously
	\begin{align*}
	\frac{d \ln (wage)}{d\ educ} = \beta_1 + \beta_2 pareduc
	\end{align*}
	hence
	\begin{align*}
	\frac{\Delta \ln (wage)}{\Delta educ} \approx \beta_1 + \beta_2 pareduc
	\end{align*}
	As for me it is difficult to say something about sign of $\beta_0$ apriori. If $\beta_2$ is positive then each additional year of parents' education yields greater average percentage change in salary with additional year of education. I cannot come up with any strong reasoning for $\beta_2$ be positive or negative.
	\item If $pareduc = 24$ then
	\begin{align*}
	100\% \cdot \frac{\Delta wage}{wage} = 4.7 \Delta educ + 0.078 \cdot 24 \Delta educ = 4.7 \Delta educ + 1.872 \Delta educ
	\end{align*}
	If $pareduc = 32$ then
	\begin{align*}
	100\% \cdot \frac{\Delta wage}{wage} = 4.7 \Delta educ + 0.078 \cdot 32 \Delta educ = 4.7 \Delta educ + 2.496 \Delta educ
	\end{align*}
	i.e. each additional year of parents' education increases by 0.078 percent points the value of percent change of salary in response to additional year of education.
	\item \begin{align*}
	t = \frac{0.0016}{0.0012} \approx 1.33
	\end{align*}
	p-value is about 0.09, hence we can reject the hypothesis on 5\% significance level. That is, the return to education does not depend on parents' education.
	\section*{Problem 3}
	
	
	\textbf{Solution}
	
	
	Since 
	\begin{align*}
	F \equiv \frac{(SSR_r - SSR_{ur}) / q}{SSR_{ur}/(n - k - 1)}
	\end{align*}
	where $SSR_r$ and $SSR_{ur}$ are sums of squares of residuals of restricted and unrestricted models respectively, $q = 2, k = 8$. 
	\begin{align*}
	R^2_{ur} = 1 - \frac{SSR_{ur}}{SST_{ur}}\\
	R^2_{r} = 1 - \frac{SSR_{r}}{SST_{r}}\\
	\end{align*}
	while
	\begin{align*}
	SST = \summa (y_i - \bar{y}) = SST_r = SST_{ur}
	\end{align*}
	hence
	\begin{align*}
	F = \frac{(R^2_{ur} - R^2_r)/q}{(1 - R^2_{ur})/(n-k-1)} = \frac{(0.232 - 0.229)/2}{(1-0.232)/(680 - 8 - 1)} \approx 1.31
	\end{align*}
	p-value is about 0.27, hence the hypothesis (that corresponding slope coefficients are 0) cannot be rejected at the 10\% significance level. That is, both these terms are jointly insignificant hence I would not include them in the model.
\end{enumerate}
\section*{Problem 4}
\begin{enumerate}[(a)]
	\item From Problem Set 2, Problem 3:
	\begin{align*}
	\hat{\beta_1} = \frac{s^2_{yx_1}s^2_{x_2x_2} - s^2_{yx_2}s^2_{x_2x_1}}{s^2_{x_1x_1}s^2_{x_2x_2} - (s^2_{x_1x_2})^2}\\
	\hat{\beta_2} = \frac{s^2_{yx_2}s^2_{x_1x_1} - s^2_{yx_1}s^2_{x_2x_1}}{s^2_{x_1x_1}s^2_{x_2x_2} - (s^2_{x_1x_2})^2}
	\end{align*}
	where $s^2_{xy}$ means a sample covariance between $x$ and $y$. That means that:
	\begin{align}\label{eq4}
	&Var(\hat{\beta_1}|x_1, x_2) = \left(\frac{s^2_{x_2x_2}}{s^2_{x_1x_1}s^2_{x_2x_2} - (s^2_{x_1x_2})^2}\right)^2 Var(s^2_{yx_1}|x_1, x_2) + \nonumber\\
	&+\left(\frac{s^2_{x_2x_1}}{s^2_{x_1x_1}s^2_{x_2x_2} - (s^2_{x_1x_2})^2}\right)^2 Var(s^2_{yx_2}|x_1, x_2) - \frac{2s^2_{x_2x_2}s^2_{x_2x_1}}{(s^2_{x_1x_1}s^2_{x_2x_2} - (s^2_{x_1x_2})^2)^2}cov(s^2_{yx_1}, s^2_{yx_2}|x_1, x_2) = \nonumber\\
	&\underset{i.i.d}{=} \left(\frac{s^2_{x_2x_2}}{s^2_{x_1x_1}s^2_{x_2x_2} - (s^2_{x_1x_2})^2}\right)^2 \summa (x_{i1} - \bar{x_1})^2Var(y_i|x_{i1}, x_{i2}) + \nonumber\\
	&\left(\frac{s^2_{x_2x_1}}{s^2_{x_1x_1}s^2_{x_2x_2} - (s^2_{x_1x_2})^2}\right)^2 \summa (x_{i2} - \bar{x_2})^2Var(y_i|x_{i1}, x_{i2}) -\nonumber\\
	&-\frac{2s^2_{x_2x_2}s^2_{x_2x_1}}{(s^2_{x_1x_1}s^2_{x_2x_2} -(s^2_{x_1x_2})^2)^2} \summa \sum_{j=1}^n (x_{i1} - \bar{x_1})(x_{j2} -\bar{x_2})cov(y_i, y_j|x_{1}, x_2) = \nonumber\\
	&=\left(\frac{s^2_{x_2x_2}}{s^2_{x_1x_1}s^2_{x_2x_2} - (s^2_{x_1x_2})^2}\right)^2 s^2_{x_1x_1} \sigma^2 +
	\left(\frac{s^2_{x_2x_1}}{s^2_{x_1x_1}s^2_{x_2x_2} - (s^2_{x_1x_2})^2}\right)^2 s^2_{x_2x_2} \sigma^2 - \nonumber\\
	&-\frac{2s^2_{x_2x_2}s^2_{x_2x_1}}{(s^2_{x_1x_1}s^2_{x_2x_2} -(s^2_{x_1x_2})^2)^2} s^2_{x_1x_2}\sigma^2 = \frac{s^2_{x_2x_2}}{s^2_{x_1x_1}s^2_{x_2x_2} -(s^2_{x_1x_2})^2}\sigma^2
	\end{align}
	Similarly for $\hat{\beta_2}$:
	\begin{align}\label{eq3}
	&Var(\hat{\beta_2}|x_1, x_2) = \frac{s^2_{x_1x_1}}{s^2_{x_1x_1}s^2_{x_2x_2} -(s^2_{x_1x_2})^2}\sigma^2
	\end{align}
	Then
	\begin{align*}
	Var(\hat{\beta_1}|x_1, x_2) - Var(\hat{\beta_2}|x_1, x_2) = \frac{\sigma^2}{s^2_{x_1x_1}s^2_{x_2x_2} - (s^2_{x_1x_2})^2} (s^2_{x_2x_2} - s^2_{x_1x_1})
	\end{align*}
	and 
	\begin{align*}
	\widehat{Var(\hat{\beta_1}|x_1, x_2)} - \widehat{Var(\hat{\beta_2}|x_1, x_2)} =\frac{\hat{\sigma}^2}{s^2_{x_1x_1}s^2_{x_2x_2} - (s^2_{x_1x_2})^2} (s^2_{x_2x_2} - s^2_{x_1x_1})
	\end{align*}
	Since $\widehat{Var(\hat{\beta_1}|x_1, x_2)} - \widehat{Var(\hat{\beta_2}|x_1, x_2)} > 0$ and the determinant of sample covariance matrix (which appears in the denominator) should be nonnegative (i.e. positive) then $s^2_{x_2x_2} - s^2_{x_1x_1}$ should be positive. Hence, once we have an invertible sample covariance matrix the sample variance of $x_1$ should be smaller than the sample variance of $x_2$.
	\item 
	\item Since
	\begin{align*}
	Var(\hat{\beta_1} + \hat{\beta_2}) = Var(\hat{\beta_1}) + Var(\hat{\beta_2}) + 2cov(\hat{\beta_1}, \hat{\beta_2})
	\end{align*}
	It remains to estimate the covariance of $\hat{\beta_1}$ with $\hat{\beta_2}$.
	\begin{align*}
	cov(\hat{\beta_1}, \hat{\beta_2}|x_1, x_2) = \frac{s^2_{x_2x_2}s^2_{x_1x_1}cov(s^2_{yx_1}, s^2_{yx_2}) - s^2_{x_2x_2}s^2_{x_2x_1}Var(s^2_{yx_1})}{(s^2_{x_1x_1}s^2_{x_2x_2} - (s^2_{x_1x_2})^2)^2} - \\
	-\frac{ s^2_{x_2x_1}s^2_{x_1x_1}Var(s^2_{yx_2}) - s^2_{x_2x_1}s^2_{x_2x_1}cov(s^2_{yx_2}, s^2_{yx_1})}{(s^2_{x_1x_1}s^2_{x_2x_2} - (s^2_{x_1x_2})^2)^2} = \\
	=\frac{-s^2_{x_1x_2}}{s^2_{x_1x_1}s^2_{x_2x_2} - (s^2_{x_1x_2})^2}\sigma^2
	\end{align*}
	That means that
	\begin{align*}
	\widehat{Var(\hat{\beta_1} + \hat{\beta_2})} = 
	\end{align*}
	\item $\summa x_ix_i^T$ is a diagonal meaning that $s^2_{x_1x_2} = 0$ hence we need to find $s^2_{x_1x_1}$ and $s^2_{x_2x_2}$. From \eqref{eq3} and \eqref{eq4} it follows that:
	\begin{align*}
	2 = SE(\hat{\beta_1}) = \sqrt{\frac{s^2_{x_2x_2}3^2}{(n-1)s^2_{x_1x_1}s^2_{x_2x_2}}}\\
	(n-1)s^2_{x_1x_1} = \frac{9}{4}
	\end{align*}
	Similarly for $\hat{\beta_2}$:
	\begin{align*}
	1 = \sqrt{\frac{s^2_{x_1x_1}3^2}{(n-1)s^2_{x_1x_1}s^2_{x_2x_2}}}\\
	(n-1)s^2_{x_2x_2} = 3
	\end{align*}
	Thus, the diagonal of $\summa x_ix_i^T$ which is $n, \summa x_{1i}^2, \summa x_{2i}^2$ is equal to:
	\begin{align*}
	100,\ \frac{9}{4},\ 3
	\end{align*}
\end{enumerate}
\section*{Problem 5}
Summary of the regression is depicted on the Fig. \ref{fig1}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{lnahe_femyrs}
	\caption{Summary of regression}\label{fig1}
\end{figure}

\begin{enumerate}[a.]
	\item Denote coefficient on $yrseduc$ as $\beta_1$, on $female\times yrseduc$ as $\beta_2$ and on $female$ as $\beta_3$. For males ($female = 0$)
	\begin{align*}
	\frac{\Delta ahe}{ahe} \cdot 100\% = \beta_1 \cdot 100 \Delta yrseduc
	\end{align*}
	i.e. the coefficient on $yrseduc$ times 100 is the percent change of average per hour salary for men adding one additional year of education, holding other things fixed.
	
	For women:
	\begin{align*}
	\frac{\Delta ahe}{ahe} \cdot 100\% = 100(\beta_1 + \beta_2)\Delta yrseduc
	\end{align*}
	i.e. the coefficient on $yrseduc$ plus the coefficient on $yrseduc \times female$ times 100 is the percent change of average per hour salary for women adding one additional year of education, holding other things fixed.
	\item 
	\item We should test whether $\beta_2 = 0$ or not. Using coeftest() we can conclude that the null hypothesis can be rejected at the 5\% significance level, i.e. the value of the aditional year of education for men and for women differs.
	\item We need to test the hypothesis that both $\beta_2$ and $\beta_3$ are equal to 0. Using linearhypothesis() we can conclude that the hypothesis should be rejected, i.e. regression lines are different for men and for women.
	\item For men additional year of education increases the average per hour salary by $8.26\%$ for women by $9.9\%$, in statistical terms this is a significant difference (since we reject the hypothesis that $\beta_2 = 0$) in real world sense I think not.
\end{enumerate}
\section*{Problem 6}

\end{document}